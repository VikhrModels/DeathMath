import yaml
from typing import List, Dict, Any, Set
import time
from pathlib import Path
import json
from datetime import datetime
from src.equality_checker import MathEqualityChecker
from src.sampler import OaiSampler
from src.mat_boy import RussianMathEval, MathDemonEval
from src.types import SingleEvalResult
from concurrent.futures import ThreadPoolExecutor, TimeoutError
from tqdm.auto import tqdm
import hashlib
import signal
import sys


class Leaderboard:
    def __init__(
        self, config_path: str, output_dir: str = "results", max_workers: int = 4
    ):
        self.config_path = config_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.max_workers = max_workers

        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.details_dir = self.output_dir / "details"
        self.details_dir.mkdir(exist_ok=True)
        self.cache_dir = self.output_dir / "cache"
        self.cache_dir.mkdir(exist_ok=True)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∏ –∫—ç—à
        with open(config_path, "r") as f:
            self.config = yaml.safe_load(f)
        self.model_links = self.config.get("model_links", {})
        self.equality_checker = MathEqualityChecker()
        self.results_file = self.output_dir / "leaderboard_results.json"
        self.results = self._load_results()

    def _get_cache_key(self, model_name: str, system_prompt: str | None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–º–ø—Ç–∞"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∫—ç—à–∞
        safe_model_name = model_name.replace("/", "_")
        cache_data = {
            "model_name": safe_model_name,
            "system_prompt": system_prompt,
            "num_examples": self.config.get("num_examples"),
            "temperature": self.config.get("temperature"),
            "max_tokens": self.config.get("max_tokens"),
        }
        cache_str = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_str.encode()).hexdigest()

    def _get_cached_result(self, cache_key: str) -> Dict | None:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            with open(cache_file, "r") as f:
                return json.load(f)
        return None

    def _save_to_cache(self, cache_key: str, result: Dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        with open(cache_file, "w") as f:
            json.dump(result, f, indent=2)

    def _load_results(self) -> Dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –∫—ç—à"""
        results = {}

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –∫—ç—à–∞
        if self.cache_dir.exists():
            for cache_file in self.cache_dir.glob("*.json"):
                with open(cache_file, "r") as f:
                    cached_result = json.load(f)
                    model_name = cached_result["model_name"]
                    timestamp = cached_result["timestamp"]
                    results[f"{model_name}_{timestamp}"] = cached_result

        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏, –¥–æ–±–∞–≤–ª—è–µ–º –∏—Ö —Ç–æ–∂–µ
        if self.results_file.exists():
            with open(self.results_file, "r") as f:
                file_results = json.load(f)
                results.update(file_results)

        return results

    def _save_results(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        with open(self.results_file, "w") as f:
            json.dump(self.results, f, indent=2)

    def _save_detailed_results(
        self, model_name: str, results: List[SingleEvalResult], timestamp: str
    ):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–∏"""
        # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        safe_model_name = model_name.replace("/", "_")
        model_dir = self.details_dir / safe_model_name
        model_dir.mkdir(exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ JSON
        details_file = model_dir / f"details_{timestamp}.json"
        with open(details_file, "w", encoding="utf-8") as f:
            json.dump(
                results, f, indent=2, default=lambda x: x.__dict__, ensure_ascii=False
            )

        # –°–æ–∑–¥–∞–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º markdown-–æ—Ç—á–µ—Ç
        markdown_file = model_dir / f"details_{timestamp}.md"
        markdown_content = self._generate_markdown_report(
            model_name, results, timestamp
        )
        with open(markdown_file, "w", encoding="utf-8") as f:
            f.write(markdown_content)

    def _generate_markdown_report(
        self, model_name: str, results: List[SingleEvalResult], timestamp: str
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç markdown-–æ—Ç—á–µ—Ç —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        md = f"# Detailed Results for {model_name}\n\n"
        md += f"Timestamp: {timestamp}\n\n"

        for i, result in enumerate(results, 1):
            md += f"## Example {i}\n\n"

            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –∏ –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –∏–∑ convo, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
            if hasattr(result, "convo") and result.convo:
                for message in result.convo:
                    if message.get("role") == "user":
                        md += f"### Task\n{message.get('content', '')}\n\n"
                    elif message.get("role") == "assistant":
                        md += f"### Model Response\n{message.get('content', '')}\n\n"

            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            if hasattr(result, "correct_answer") and result.correct_answer:
                md += f"### Correct Answer\n{result.correct_answer}\n\n"

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
            if (
                hasattr(result, "extracted_answer")
                and result.extracted_answer is not None
            ):
                md += f"### Extracted Answer\n{result.extracted_answer}\n\n"

            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫—É
            if hasattr(result, "score") and result.score is not None:
                md += f"### Score\n{result.score}\n\n"

            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            if hasattr(result, "tokens"):
                md += f"### Tokens Used\n{result.tokens}\n\n"

            md += "---\n\n"

        return md

    def evaluate_model(
        self, model_name: str, system_prompt: str = None
    ) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å"""
        cache_key = self._get_cache_key(model_name, system_prompt)
        cached_result = self._get_cached_result(cache_key)

        if cached_result is not None:
            if self.config.get("debug"):
                print(f"\nUsing cached result for {model_name}")
            return cached_result

        if self.config.get("debug"):
            print(f"\nEvaluating {model_name}")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_model_name = model_name.replace("/", "_")

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
        temp_config = self.config.copy()
        temp_config["model_list"] = [model_name]
        if system_prompt is not None:
            temp_config[model_name]["system_prompt"] = system_prompt

        temp_config_path = self.output_dir / f"temp_config_{safe_model_name}.yaml"
        with open(temp_config_path, "w") as f:
            yaml.dump(temp_config, f)

        try:
            sampler = OaiSampler(str(temp_config_path))
            evaluator = RussianMathEval(
                equality_checker=self.equality_checker,
                num_examples=self.config.get("num_examples", None),
                debug=self.config.get("debug", False),
            )

            start_time = time.time()
            results = evaluator(sampler)
            evaluation_time = time.time() - start_time

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._save_detailed_results(model_name, results.results, timestamp)

            total_tokens = sum(
                r.tokens for r in results.results if hasattr(r, "tokens")
            )

            model_result = {
                "model_name": model_name,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è
                "score": results.score,
                "total_tokens": total_tokens,
                "evaluation_time": evaluation_time,
                "system_prompt": system_prompt,
                "timestamp": timestamp,
                "cache_key": cache_key,
            }

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            self._save_to_cache(cache_key, model_result)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª—é—á–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.results[f"{model_name}_{timestamp}"] = model_result
            self._save_results()

            return model_result

        finally:
            temp_config_path.unlink(missing_ok=True)

    def evaluate_model_parallel(self, args: tuple) -> Dict[str, Any]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–¥–Ω—É –º–æ–¥–µ–ª—å (–¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ ThreadPoolExecutor)"""
        model_name, system_prompt = args
        return self.evaluate_model(model_name, system_prompt)

    def _get_measured_models(self) -> Set[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–∂–µ –∏–∑–º–µ—Ä–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –∫—ç—à–∞"""
        measured_models = set()
        if self.cache_dir.exists():
            for cache_file in self.cache_dir.glob("*.json"):
                with open(cache_file, "r") as f:
                    cached_data = json.load(f)
                    measured_models.add(cached_data["model_name"])
        return measured_models

    def evaluate_all_models(self, system_prompts: Dict[str, str] = None) -> None:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞"""
        if system_prompts is None:
            system_prompts = {}

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–∂–µ –∏–∑–º–µ—Ä–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        measured_models = self._get_measured_models()

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
        config_models = set(self.config["model_list"])

        # –ù–∞—Ö–æ–¥–∏–º –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        new_models = config_models - measured_models

        if new_models:
            print(f"\nFound new models to evaluate: {', '.join(new_models)}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫—ç—à–∏ –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        for model_name in config_models:
            if model_name in measured_models:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫—ç—à –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏
                for cache_file in self.cache_dir.glob("*.json"):
                    with open(cache_file, "r") as f:
                        cached_data = json.load(f)
                        if cached_data["model_name"] == model_name:
                            key = f"{model_name}_{cached_data['timestamp']}"
                            self.results[key] = cached_data
                            break

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        if new_models:
            uncached_args = [
                (model_name, system_prompts.get(model_name))
                for model_name in new_models
            ]

            print(f"\nEvaluating {len(uncached_args)} new models...")

            def handle_sigint(signum, frame):
                print(
                    "\nGracefully shutting down... Please wait for current evaluations to complete."
                )
                executor.shutdown(wait=True)
                sys.exit(0)

            original_sigint = signal.getsignal(signal.SIGINT)
            signal.signal(signal.SIGINT, handle_sigint)

            try:
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    # –°–æ–∑–¥–∞–µ–º futures –¥–ª—è –≤—Å–µ—Ö –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
                    futures = [
                        executor.submit(self.evaluate_model_parallel, args)
                        for args in uncached_args
                    ]

                    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —Å leave=True, —á—Ç–æ–±—ã –æ–Ω –æ—Å—Ç–∞–≤–∞–ª—Å—è –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                    pbar = tqdm(
                        total=len(futures), desc="Evaluating new models", leave=True
                    )

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π future –ø–æ –º–µ—Ä–µ –µ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                    # –≤–º–µ—Å—Ç–æ –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø–æ futures, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
                    completed = 0
                    while completed < len(futures):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–∞–∂–¥–æ–≥–æ future
                        for i, future in enumerate(futures):
                            if future.done() and not hasattr(future, "_processed"):
                                try:
                                    result = future.result(timeout=1)
                                    if result:
                                        key = f"{result['model_name']}_{result['timestamp']}"
                                        self.results[key] = result
                                        # –°—Ä–∞–∑—É —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à
                                        self._save_to_cache(
                                            self._get_cache_key(
                                                result["model_name"],
                                                result.get("system_prompt"),
                                            ),
                                            result,
                                        )
                                    # –û—Ç–º–µ—á–∞–µ–º future –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π
                                    setattr(future, "_processed", True)
                                    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞
                                    completed += 1
                                    pbar.update(1)
                                except TimeoutError:
                                    print(
                                        "\nWarning: Evaluation timed out for one of the models"
                                    )
                                except Exception as e:
                                    print(f"\nError during evaluation: {str(e)}")
                                    # –û—Ç–º–µ—á–∞–µ–º future –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                                    setattr(future, "_processed", True)
                                    completed += 1
                                    pbar.update(1)

                        # –ù–µ –Ω–∞–≥—Ä—É–∂–∞–µ–º CPU –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å—Ç–∞—Ç—É—Å–∞
                        time.sleep(0.1)

                    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
                    pbar.close()

            finally:
                signal.signal(signal.SIGINT, original_sigint)
                self._save_results()
        else:
            print("\nNo new models to evaluate, using cached results")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        missing_models = config_models - set(
            result["model_name"] for result in self.results.values()
        )
        if missing_models:
            print(f"\nWarning: Missing results for models: {', '.join(missing_models)}")

        self._save_results()

    def generate_markdown(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç markdown —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        md = "# Math Evaluation Leaderboard\n\n"
        md += f"Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
        md += "| Model | Score | Tokens Used | System Prompt | Evaluation Time | Details | Model Info |\n"
        md += "|-------|--------|-------------|---------------|----------------|----------|------------|\n"

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º –∏ –±–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –∫–∞–∂–¥–æ–π
        model_best_results = {}
        for result in self.results.values():
            model_name = result["model_name"]
            if (
                model_name not in model_best_results
                or result["score"] > model_best_results[model_name]["score"]
            ):
                model_best_results[model_name] = result

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ score
        sorted_results = sorted(
            model_best_results.values(), key=lambda x: x["score"], reverse=True
        )

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
        for result in sorted_results:
            model_name = result["model_name"]
            system_prompt = result["system_prompt"] or "None"
            if len(system_prompt) > 30:
                system_prompt = system_prompt[:27] + "..."

            details_link = (
                f"[Details](details/{model_name}/details_{result['timestamp']}.md)"
            )

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
            model_info = ""
            if model_name in self.model_links:
                model_info = f"[üìö]({self.model_links[model_name]})"

            md += f"| {model_name} "
            md += f"| {result['score']:.3f} "
            md += f"| {result.get('total_tokens', 0)} "
            md += f"| {system_prompt} "
            md += f"| {result['evaluation_time']:.1f}s "
            md += f"| {details_link} "
            md += f"| {model_info} |\n"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º markdown
        with open(self.output_dir / "leaderboard.md", "w") as f:
            f.write(md)

        return md

    def evaluate_math_demon_subsets(self):
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≤—Å–µ –ø–æ–¥—Å–µ—Ç—ã –∏–∑ MathDemon_Demidovich"""
        subsets = [
            "Approximation_by_Polynomials",
            "Continuous_Functions",
            "Convex_Functions",
            "Differentiation",
            "Improper_Integrals",
            "Infinite_Series",
            "Integration",
            "Sequences_and_Limits",
            "Series_of_Functions",
        ]

        for subset in subsets:
            print(f"\nEvaluating subset: {subset}")
            evaluator = MathDemonEval(
                subset_name=subset,
                num_examples=self.config.get("num_examples", None),
                debug=self.config.get("debug", False),
            )

            sampler = OaiSampler(self.config_path)
            results = evaluator(sampler)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Å–µ—Ç–∞
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self._save_detailed_results(
                f"MathDemon_{subset}", results.results, timestamp
            )

            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –æ–±—â–∏–π leaderboard
            self.results[f"MathDemon_{subset}_{timestamp}"] = {
                "model_name": f"MathDemon_{subset}",
                "score": results.score,
                "total_tokens": sum(
                    r.tokens for r in results.results if hasattr(r, "tokens")
                ),
                "evaluation_time": results.evaluation_time,
                "timestamp": timestamp,
            }

        self._save_results()


def main():
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    leaderboard = Leaderboard("configs/run.yaml")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–Ω—ã–µ system prompts –¥–ª—è –º–æ–¥–µ–ª–µ–π
    system_prompts = {
        "gpt-4-1106-preview": "You are a helpful math assistant. Answer in Russian.",
        "gpt-3.5-turbo-0125": "Solve math problems step by step. Answer in Russian.",
        "gpt-4o-mini": "You are a math expert. Provide detailed solutions in Russian.",
    }

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ system prompts
    leaderboard.evaluate_all_models(system_prompts)

    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º markdown —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    md = leaderboard.generate_markdown()
    print("Leaderboard generated!")
    print(md)


if __name__ == "__main__":
    main()
